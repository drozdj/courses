# [The Engineer’s Interpretability Sequence](https://www.alignmentforum.org/s/a6ne2ve5uturEEQK7) (Stephen Casper)

Updated: 02.08.24

Status: Draft

---
> "The most common way to evaluate a model is by its performance in some test set or environment. But test sets alone can fail to reveal – and often incentivize – undesirable solutions involving overfitting, biases, deception, etc."
* do we have ways to check that an AI is 'well-behaved' because it knows its in a *simulation*?

* the main point of the series is to showcase the jump from the *toy* scenarios in which Interpretability has been presented towards actual real-world application -- we have made *significant* strides, but not (m)any of which have been *actually usefull*.

* Stephen makes the argument that sometimes we get caught up trying to do work exclusively in a certain field (ie mech interp) and that quickly we can find ourselves not persuing more obvious/easier solutions because we feel the need to stay in the field. this he argues is a big mistake, for just because we are a 'mech interp eng' does not mean that solving problems which such particular tools is how we should think about it. he therefore proposes that we should find a better definition / drop it entirely.
    * > interpretability: “any method by which something novel about a system can be better predicted or described.”

* why might Stephen believe that "outer alignment work is sufficiently different" from interpretability?
    * outer alignment: the process in which we instill human values into the AI via methods like Constitutional-AI
    * interpretability: understanding what 'thought-process' the AI had when coming up with its answer

* touches on how the research in interpretability is *mostly* cherry-picked examples, and that the authors often treat their hypotheses as conlusions.
    * a strong effort, push should be to break this apart. it is clear that everybody wants to publish research and they race to 'who can do the most' not paying adequate attention to the impact of the results they present. this thus wastes readers time as we have to sift through a bunch of garbage to (fingers-crossed) find something valueble
        * a method to solve this would be (i) desolving the ego of researchers, (ii) creating good research, by priding ourselves on 
        creating rigorous tests to try to tear the hypothesis to shreds, and when that doesnt work, try some more! 
