# AI Watermarking Won't Curb Disinformation by Jacob Hoffman-Andrews (2024)

Updated: 31.07.24

Status: Completed

---
ideas

What if we could create layers of these watermarks? (or better yet) what if this was paired with cryptography to decode the layers that have been applied to the image/video?

---
* >"This newest study goes even further. While some researchers have held out hope that visible (‚Äúhigh perturbation‚Äù) watermarks might be developed to withstand attacks, Feizi and his colleagues say that even this more promising type can be manipulated."
    * Q: what does [Invisible Image Watermarks Are Provably Removable Using Generative AI](https://www.arxiv.org/pdf/2306.01953) show that low-pertubation (less visable to eye) is much less effective than high-pertubation watermarking?
    * Q: what is a destructive attack?
    * Q: what is a constructive attack?
    * >"To validate our theory empirically, we conduct extensive experiments on five widely used invisible watermarks [23, 25,27,28,29] including the ones currently used by the popular open-source image generative model Stable Diffusion." 
        * thats no good, even SOTA (Stable Diffusion) is unable to protect against watermark removals...

* >Bars Juhasz, the cofounder of Undetectable
    * well this guy is sort of an ass isn't he.. hes offering services for bypassing ai content detection.
    * Q: what are his intentions for doing so?
        * after looking over the page, i noticed that the financially its a brilliant business plan, just morally for him probably not so much

* > ‚ÄúThere will always be sophisticated actors who are able to evade detection,‚Äù Goldstein says. ‚ÄúIt‚Äôs OK to have a system that can only detect some things.‚Äù He sees watermarking as a form of harm reduction and useful for catching lower-level attempts at AI fakery, even if it can‚Äôt prevent high-level attacks."
    * i believe Goldstein to be widly mistaken. consider this, a larger company takes this advice and creates exactly what he is proposing, 'a ai detector... that is only about 90% correct.' they release this to the public and receive great praise from the communities and teachers. *tada*, you have imprisoned many for no reason, you have failed many students for frogery, you have *amplified* human biases that used this system as a sanity-check! & etc.
    * > DeepMind is careful to hedge its bets, [noting](https://deepmind.google/discover/blog/identifying-ai-generated-images-with-synthid/) that the tool ‚Äúisn‚Äôt foolproof‚Äù and ‚Äúisn‚Äôt perfect.‚Äù
        * OH SUGAR. don't you do it DeepMind. dont you even dare! üòÖ

* > "Feizi is largely skeptical of the idea that watermarking is a good use of resources for companies like Google. ‚ÄúPerhaps we should get used to the fact that we are not going to be able to reliably flag AI-generated images,‚Äù he says."
    * > "*largerly skeptical*"
        * you mean to tell we we build this entire article off of you & then you tell me you are hopeless yourself? lol
    * it looks like a cat and mouse game. defenses get stronger, the stronger attackers get. 

* > "they are more likely to flag writing as AI-created when the word choice is fairly predictable and the sentences are less complex"
    * do we leave the math-genius suffering because his literacy is not on par with the other folk in the class?
    * what about the younger grades that have not yet developed comprehensive writing skills?
    * what about the foreign student?
    * what if the detection model was trained on a llm whose dataset included disproportianately many student example?
    * what if the students writing style happens to be similiar to the *mean* of training examples found in dataset?
* [similar example to this](https://www.eff.org/deeplinks/2022/05/podcast-episode-teaching-ai-its-targets) i had just come across is the bias on predicting school-shooters in florida (of which ~60% of students are black/hispanic). the argument given was that if the dataset contains disproportianate populations then the predictions will be skewed.
    * i however cannot agree with this case, (via statistics) are you not able to standardise the dataset in a way to randomly sample an equivilant amount of each 'group'?

[SynthID (DeepMind)](https://deepmind.google/technologies/synthid/)
* >"The watermark is robust to many common modifications such as noise additions, MP3 compression or speeding up and slowing down the track."
    * has it ever occured that maybe attacker(s) (ie. a country *wink wink*) may try to spread mis-information via embedding real images with AI generated watermarked ones?
        * this would cause confusion, distrust and turmoil in society which is the very thing they want!

* [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165)
    * >In reference to GPT-3 (2020, Jul) "according to a technical paper. Scooped up in this data is some of the personal information you share about yourself online. This data is now getting OpenAI into trouble."
        * Italian Garante was concerned about ChatGPT collecting information about users **interacting** with the bot as well as age verification (<13y shouldn't be allowed), and **not** what i thought which was data in dataset.<br>
        *Oops* :P. **the below thread is therefore scrapped but i will keep it to remind myself of my thought process**
        
        * ~~lets prove this~~
            * datasets used for GPT-3 (2020, Jul)
                * CommonCrawl
                * expanded version of the WebText dataset üëà
                    * *aha!* here you are! this contains peronal information (even though there are measures to protect user privacy)
                * English-language Wikipedia
                * Books1 & Books2
            * although i cannot agree with OpenAIs approach to this, it seems highly unlikely that they were training intentially on *personal* data.
                * this would just make life more difficult for them! now they have to go the extra step of finetuning all of that rubbish out, to which they are back at the model they would have initially had without the personal information (given that maybe ~5% increase due to the context-based structure that it may have extracted from looking over all of that data.)
            * > "However, we have found that unfiltered or lightly filtered versions of Common Crawl tend to have lower quality than more curated datasets. Therefore, we took 3 steps to improve the average quality of our datasets: (1) we downloaded and filtered a version of CommonCrawl based on similarity to a range of high-quality reference corpora"
                *

* > However, GPT-4‚Äôs technical paper includes a section on privacy, which says its training data may include ‚Äúpublicly available personal information,‚Äù which comes from a number of sources. The paper says OpenAI takes steps to protect people‚Äôs privacy, including ‚Äúfine-tuning‚Äù models to stop people asking for personal information and removing people‚Äôs information from training data ‚Äúwhere feasible.‚Äù
    * so what happend? <br> even after the Italy case, where OpenAI in 2020 released GPT-3 and removed the personal information of many people, they came back in the future and done the very *same thing* with ChatGPT & GPT-4... But this time... but this time with a wellbehaved version because they improved RLHF to train the model to not dox, give harmful information, etc.
    * Q: Did GPT-3 released in 2020 have InstructGPT (assistant model) at that time or any other types of RLHF?
        * no, it only had finetuning
        * Q: whats the difference between finetuning & RLHF?
            * 1). RLHF is built ontop of finetuning.
            * 2). finetuning is the act of generating high-quality responses to how the model should structure its responses to queries. ex. you ask the model, ME:"What is the capital of poland?", MODEL:"Warsaw". That gives the answer, but it doesnt give any other general information which the user would likely follow up on, it should respond something like, MODEL:"Warsaw is the capital of poland! It has a population of _, & is actually the largest tourist destination in Europe for the 10th year running!" 
            * 3). RLHF is where the model produces some outputs and a human-expert decides which of these are the *best*. (of course, if they all suck, then either the fine-tuning stage needs more attention or the model should be flushed üöΩ)


---
## References
* https://www.wired.com/story/artificial-intelligence-watermarking-issues/
* https://www.eff.org/deeplinks/2023/11/best-serve-students-schools-shouldnt-try-block-generative-ai-or-use-faulty-ai 
* https://slate.com/technology/2023/12/ai-generated-birds-santa-cardinal.html
* https://futurism.com/trump-ai-voice-cloned-fake-video-anderson-cooper
* https://deepmind.google/technologies/synthid/
* https://edition.cnn.com/2023/08/19/tech/schools-teaching-chagpt-students/index.html
* https://www.wired.com/story/italy-ban-chatgpt-privacy-gdpr/

