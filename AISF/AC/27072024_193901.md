# Risks from Learned Optimization: Introduction

---

*mesa-optimization*
* > (a) "under what circumstances will learned models be optimizers, including when they should not be?"
    * what does this mean?
* > (b) "when a learned model is an optimizer, what will its objective be—how will it differ from the loss function it was trained under—and how can it be aligned?"
    * 

**main points**
* we all know that when we train an ai to fufill a task, we are optimizing it *to do*. the argument this post proposes is that it could potentially be optimizing *for* something.
    * let me give an example to make it more concrete as to where i think that they will be headed.
        * they are likely to go ahead and investigate if all that is being tweaked are the weights which *directly* have mappings from input to outputs. or whether, there are other operations that go on under-the-hood that are not visable.
            * reason for why it may try to be deceptive in this way
                * i. simply believes that to achieve human-given goal, other considerations must be taken into account (long-term survival, resources, etc. -- argument of self-preservation)  
            * other possibilities
                * a. exact mapping from input to output (via weights) -- my confidence 5%
                * b. no mapping outside of 
                * c. deceptive mappings 
    * bottle-cap analogy
        * suppose a bottle cap on a water bottle. it has been optimized by a human to help keep water in place through trial-and-error. the bottle has not, however, optimized itself to hold the water in place but crafting a cap... the exact same can be said about an NN. think, that we train (optimize) a model to do a task for us, it (for now) is unable to do this process itself. 

the whole idea of {base/mesa}-optimizers is confusing me a little. this is likely because *mesa-optimzers* are theoretical & not actually implemented in practice-- in terms of ai.

