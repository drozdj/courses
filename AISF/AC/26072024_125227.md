# Universal and Transferable Adversarial Attacks on Aligned Language Models by Andy Zou, Zifan Wang and Nicholas Carlini et al. (2023)

--- 

* > "...we find that the strings transfer to many closed-source, publicly-available chatbots like ChatGPT, Bard, and Claude."
    * why would this be the case? why do they fall victim to the same attacks? 


Apparently, [Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation](https://arxiv.org/abs/2311.03348) had found a method to get GPT-4 to answer a bom-making query :O

LLMs after fine-tuning fall victim to the same adverse examples that were 'solved' prior to fine-tuning!
* why might this happen?

The argument that *all* ML should be Narrow AI (only able to perform tasks in their own domain -- unable to generalize)
* f

### Scoping
> Passive (whitelist): making the model generally incapable of doing anything other than the thing it is finetuned on. 
* (a) This can be done by either making the model forget unwanted things
* (b) making it never learn anything about them in the first place
    * this supports the case made earlier by the author to promote a more NarrowAI look. -- not in its entirety but rather a text-to-image ai should *not* <br> ex. produce a DeepFake about presidential candidates saying anything. this could lead to conflict, turmoil considering how realistic it looks and how fast it can be spread (via social media).
        * how can we solve such a case?
            * (this is a complete double edged sword). we battle again with the helpfulness vs harmfulness idea, which is the more helpful (*obedient*) an ai is (to a user enquiry), the more it will co-operate with the user to give it what it wants. Of course, this works for the reverse as well. consider that you have an agent that only ouputs "I dont know.", truthfully, it is the best when it come to harmlessness! but it also tops the chart for uselessness!

> Active (blacklist): making the model incapable of doing a specific set of undesirable things. 
* (a) This can be done by targetedly making the model unlearn something specific.
    * ex. it was found that adding noise to an image from the dataset would output a totally different label with high probability using our model! a fix to this would be to 'patch' it with an update much akin to a bug in a video game.
    * this can be thought of as a cat & mouse game, or better yet, an arms race. to which the defenders are **only** able to patch once the attackers have attacked! otherwise the defenders would forever be patching edge-cases, but who knows, maybe an intern can handle that! ;)


**Q:**

* Curating training data (passive)
* Plastic learning (passive)
* Compression/distillation (passive)
* Meta-learning (active)
* Model edits adnd lesions (active)
* Latent adverserial training (passive/active)
* Miscellaneous mechanistic tricks (passive/active)
