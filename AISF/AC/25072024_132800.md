# Constitutional AI: Harmlessness from AI Feedback (Bai et al.) (2022)

--- 
* motivations for the paper
    * (1) to study simple possibilities for using AI systems to help supervise other AIs, and thus scale supervision, 
    * (2) to improve on our prior work training a harmless AI assistant by eliminating evasive responses, reducing tension1 [Bai et al., 2022, Glaese et al., 2022] between helpfulness and harmlessness and encouraging the AI to explain its objections to harmful requests,
    * (3) to make the principles governing AI behavior, and their implementation, more transparent, and 
    * (4) to reduce iteration time by obviating the need to collect new human feedback labels when altering the objective. Let us discuss these motivations in more detail. 
---
* > "Consitutional AI"?
    * we **only** provide the model with a list of *values*, *rules* & *princples*
    * importance of constitution
        * > We chose the term ‘constitutional’ because we are able to train less harmful systems entirely through the specification of a short list of principles or instructions, i.e. a constitution. But we are also employing this terminology to emphasize that when developing and deploying a general AI system, we cannot avoid choosing some set of principles to govern it, even if they remain hidden or implicit.
            * the point being made is that we still need humans need to provide *some* guidance initially. 

* rather than human feedback on dangers, the goal is for many ai's self-improve and have the ability to highlight dangers in each other and thus eliminate them.
    * this of course means that they are not trained on the initial data, because if that were the case, then the ai's would not do such behaviour in the 1st place! and 2nd, they would be prone to the very same blind-spots if they did exist!

* the process in which we use both a) SL then b) RL is a tad bit confusing, clarify this all later.

* > "The method therefore improves upon, and partially replaces reinforcement learning from human feedback [Christiano et al., 2017]."
    * how are we actually  

* > "The new assistant ‘RL-CAI’ is preferred by crowdworkers over those trained with previously collected [Bai et al., 2022, Ganguli et al., 2022] human feedback labels for harmfulness."

* > "...to improve on our prior work training a harmless AI assistant by *eliminating evasive responses*, reducing tension"
    * (the way in which this is phrased) is as though the authors have had previous work of which they could produce RLAIF successfully to give feedback. however, struggled to get the recipient agents to cooperate.
