# Intro to Large Language Models (Andrej Karpathy) (2023)

--- 
## free-recall

**Q: what is the 2-stage process? (& why might there be 3?)**
* 1. pre-training - 'base' model
    * 1. scrape gigantic amounts of data (dependent on quality needed, web/textbooks)
    * 2. train a model (with an underlying architecture) to capture all of the patterns that emerge -- 6,000 GPUs required & many months for SOA 
    * use final model -- it will likely output correct answers, but it will not do so in a reliable way. its answers may include the correct content but not in a optimal human-readable format, which leads onto stage2.
* 2. fine-tuning - 'assistant' model
    * you guide the 'base' model with how it should actually respond to queries. you generate a bunch of 'expert' questions, then you provide correct answers for which the model will learn to use as a template and generalize from. 
        * ex. who was the 45th president of USA? <br> Oh blimey! That must have been the polarizing Donald J. Trump! You either hate him or love him, he had been in presidency from 2017, 2021.
            * you can see that there is a sense of humour and wit about this model.
* 3. reinforcement learning from human feedback (RLHF)
    * say we prompt the LLM "write short poem about a lion that transformed to a man".
        * it generates many examples of its idea of the above, and we pick the most optimal one. in this sense, we don't have to give a 'expert' answer but rather use it to feedback loop on itself.

**Q: how does Karpathy recommend LLMs to be viewed (as opposed to chatbots)?**
* Karpathy recommends that they be viewed as akin to OS, for which we have MacOS, Windows, Linux (& its many derivatives).
    * similar to OS, there are many security issues with LLMs which will be covered below.
    * LLM does not simply utilize one tool. rather it has an arsenal of tools (browser, calculator, terminal, file system, etc) & it applies whichever is context appropriate.

**Q: what is system 1 vs 2 thinking?**
* System 1 --
    * short & easy tasks -- cached
        * 3+3 
* System 2 --
    * difficult tasks 
        * 53*3
* currently, LLMs are only doing the 1st. they are just *spewing* out the first-thing that comes to mind (based on a probability distribution of likelihood). they are unable (even given the direct instruction) to **think** through why they have actually given the answer they have.

**Q: AlphaGo & self-improvement.**
* it shouldn't come as a suprise that a model which is trained on (even world class) humans on a given task, does **not** outperform the best. rather, i better approach is to (once that top level has been reached) to place it in an environment where it can play against itself & learn! this has been revolutionary and the field it called self-play.

**Q: Jailbreaks**
* Prompt Injection
* Data poisoning / Backdoor attacks ("sleeper agent" attack)
* as mentioned previously, similar to OS security issues, LLMs suffer from attacks too.
    * it is generally understood that LLMs can provide accurate suggestions and intruction in any context (given that it was exposed to it in its dataset).
        * we can see how this can go wrong very quickly, in the sense that terrorists can use it for their benefit or (other) ill-minded people.
--- 
* suprisingly, unless prompted from the specific angle that had occured in the dataset, the LLM will not be able to give answer (although, presented in a different variation, gives correct with perfect confidence) 
    * consider the center ball as the model, and blue arrows the frequency of query appearences in dataset.
        * we can think of the red area as the inverse, and although the context is the same, the angle of which is being tested it different. 
            * ![](/data/24072024_000236.png)
