# Scott Aaronson, 2022, “Reform AI Alignment”, Shtetl-Optimized blog. Also, Boaz Barak and Ben Edelman, 2022, “AI will change the world, but won’t take it over by playing ‘3- dimensional chess’”

Updated: 01.08.24 10:23 AM

Status: Draft

---
> "We believe that this is uncontroversial - for example, it’s not far-fetched to claim that AI would make much better chess players than kindergarten teachers."
* it is not immediately obvious to me that this is the case.
    * consider, 

> "An AI engineer will be much more useful than an AI CEO (see also Table 2)"
* im struggling hard to conceive this as true. the point the author tries to make is that of AIs being effective at short-term tasks (with what i understand to be, tight feedback loops. chess, image recognition, etc). and long-term tasks, running a company / teaching. it doesn't quite sink in how the long-term tasks are not just a agglomeration of various *different* short-term tasks which can be many narrow-AIs with say, a 'master' AI that forms the right decision. Of course, having a human do this would be a more efficient idea, but what about tasks such as crypto mining which humans may not want as its boring?

> "Even if a long-term AI system is built, it will likely not have a significant advantage over humans assisted with short-term AIs."
* the author tackles my take on ai 'masters'.
* im unsure that this is the above is true. consider that "humans" in the context presented actually were of median intelligence, in several 

> Arguably, the fact humans are far better than chimpanzees at culturally transmitting knowledge is more significant than the gap in intelligence between individuals of the two species.
* what on earth. my mind 🤯
    * i struggle to see how if chimpanzees were able to "culturally transmit knowledge" well, how that would play out. and somehow i do struggle to visualize that it would be very different.
    * what does it mean to "culturally transmit knowledge"?
        * how about those that spent entire lives in isolation yet have produced amazing feats of work? (by societal standards) 

> "The brilliance of individuals like Newton may have been crucial for speeding up the Scientific Revolution, but there have been brilliant individuals for millennia. The crucial difference between Newton and Archimedes is not that Newton was smarter, but rather that he lived at a later time and thus was able to stand on the shoulders of more giants. As another example, a collection of humans, aided by Internet-connected computers, can do much better at pretty much any intelligence feat (including but not limited to IQ exams) than any single human."
* what a great point!

> "Also, the only way to accrue the benefits of the strategy would be to continue pursuing it in the long term. Hence users would have to trust the AI and follow its recommendations blindly. For example, think of the case in Chess where an AI figures out that the best move is to sacrifice the queen because for any one of the possible opponent’s moves, there is a countermove, and so on and so forth. The only explanation for why this strategy is a good one may consist of an exponentially big game tree up to a certain depth."
* > "There is an alternative viewpoint, which is that an AI CEO would basically be equivalent to a human CEO but with superhuman “intuition” or “gut feeling” that they cannot explain but somehow leads to decisions that yield enormous benefits in the long term. While this viewpoint cannot be ruled out, there is no evidence in current deep learning successes to support it."
    * it is important to notice that we do not know when that *wild* decision (such as the queen sacrifice) will converge in the real world, in the space of a classical chess game it can be anywhere <120minutes. but in the real world, it could be decades if not centuries, or millennia... and what happens if it gathers valuble data which *changes* its worldview? then how does that queen sacrifise play out? (because we know it didn't simply brute-force search all the possiblities, *ever*...?)

> "Indeed, by far the most exciting advances for deep learning have not been through reinforcement learning, but rather through techniques such as supervised and unsupervised learning. (With the major exception being games like Chess and Go, though even there, given the success of non-RL engines such as Stockfish versions 12 and later, it is not clear RL is needed.)" 
* 😮
> "The real world (unlike the game of chess or even poker), involves a significant amount of unpredictability and chaos, which makes highly elaborate strategies depending on complex branching trees of moves and counter-moves far less useful."
* i think what the author is getting at in this entire paper is something akin to [The Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html). however, (ironically) the author of Bitter Lesson is Richard Sutton, a pioneer of RL even writing a textbook about it!
    * they do both believe that computation is the core idea. Boaz Barak believes that **brute-force** is what AI is best at and it should stay this way (supervised, unsupervised). and that abstracting beyond that in non-narrow environments where goals can be misaligned (RL) should be dropped altogethe, he makes the claim that all the notable accomplishments in RL (AlphaZero Chess, Go) could have very well been done in supervised learning narrow environments (Stockfish Version >=12 ).

> " For example, while chess experts often find it hard to understand why an engine such as AlphaZero makes a specific move, by the end of the game, they often understand the reasoning retrospectively and the sub-goals it was pursuing."
* could this possibly mean that chess experts are able to learn/replicate these moves (from the AI agent) in their own future games? or has AlphaZero just found tons of edge-cases/long-tail where they would lack application outside of that specific context?


> "However, what we call “short-term AI” encompasses much more than “Tool AI”, and absolutely includes systems that can take actions such as driving cars, executing trading actions, and so on and so forth."
* Boaz seperates long-term and short-term tasks as such: its better to start with long-term for clarity, if it includes strategy, future planning, structure (template) for how/when to execute on things then it falls under this category. if it consists of tasks that can be seen in isolation such as self-driving, chess, music-production, no matter how small or large the task, it falls under short-term AI.

> "Why is Hypothesis 2 necessary for the “loss of control” scenario? The reason is that this scenario requires the “misaligned long-term powerful AI” to be not merely more powerful than humanity as it exists today, but more powerful than humanity in the future. Future humans will have at their disposal the assistance of short-term AIs."
* the argument is as follows. firstly, 'powerful AI' in this paper refers to above human intelligence. the point that Boaz is making is that we should not base this measure as of todays baseline, rather the *heavily amplified* version that any given human will have be when they have access to short-term AIs in most domains (if the field continues on this trajectory).

> **Loss-of-Control Hypothesis 2:** In several key domains, *only* AIs with long-term goals will be powerful.
* of course, this is the reasoning for the author arguing that we should protect this from happening in the first place and just stick to short-term AIs 

* Boaz paints this beautiful analogy for which he shows that a common theme amongst complex systems is that no matter how much more intensive they are from each other, they eventually converge to a mean the further the prediction. unfortunetely, i will have to return to this to capture the analogy he is making because im unable to see it with the weather prediction graphs that he has provided.
    * > "In a variety of human endeavors, it seems that the cognitive skills needed to make decisions display a similar phenomenon. Occupations involving making decisions on the mid-range horizon, such as engineering, law, and medicine, require higher cognitive skills than those requiring long-term decisions such as CEOs or Politicians (see Table 3)."
        * that clears it up a little, so the point Boaz makes is that you have to be more intelligent to make predicitions in the mid-range over that of those in the long-term?
            * but how can this be the case?
                * what makes a good long-term thinker then if it isn't intelligence?
                    * bayesian thinking?

Claim 2: It may be possible to extract powerful short-term modules from long-term systems.
* Stockfish 12 (which beats AlphaZero! -- SOTA in RL) has "inspired advanced" from some components of AlphaZero.
    * Q: what exactly did Stockfish learn from AlphaZero?
* then randomly jumps to speaking of 'verification'? how does this even relate to what we are learning?
* then about quantum mechanics being taught to undergrads? what point is he trying to make here?

> "AI software developer system is trained to maximize the reward for each task separately, as opposed to maximizing the sum of rewards over time over adaptively chosen inputs of its own making."

Boaz claims in summary:
* our time should be spend on *hardening* the short-term AIs to make them robust to hacking, misinformation, etc.
* > "Regardless, we believe that more research needs to be done on understanding the internal representations of deep learning systems, and what features and strategies emerge from the training process (so we are happy that the AI safety community is putting increasing resources into “interpretability” research)"
    * 👍 feels great about (mech.?) interpretability as well!

[Paul Christiano Response][0] -- (this is my interpretation of his comment)
* very insightful. Paul highlights the differences in AI systems and why the we might seek the riskier option.
    * suppose that there is an AI musician, which writes, composes, publishes, interacts with fans & more. the same way in which you would have a human today persuing the same career. now think of yourself as their manager, you main goal is to be their assistant, organize their meetings and ensure they are not exploited (by 360-deals 😋). the funny thing is that the AI actually has some experience in the music field (much more than you), because it had a career in it that failed previously.
        * **process-based**: we get the AI musician to summarize what its been doing to further its career and we recieve 'logs' from it with summaries of why/how we should take actions. remember, you are only here to assist, remember, the ai has more experience than you in this field.. so you gather up all of the summaries that it has provided and you give it the green light to proceed. great.
        * **outcome-based**: the AI forms its decisions, then implements them itself without your assistance. this creates a awkward dynamic between the two of you, becasuse you are now supposed to assist but in what way? well, you gather up the feedback from how well that action played out in the real-world, monitor it, and then tell the AI how it can improve/ what changes it can make.
        * its very clear that in the **outcome-based** scenario we are now introducing the idea of deception, partial-truths (white lies -- a sugarcoated lie is as good as an outright lie) and confict in the relationship. of course, the feedback loop on outcome-based approach is now very tight, the AI can compute and come to decisions faster. however, we must consider that this *significant* improvement in progress has with it *significant* drawbacks. 

---
## Key points
* short-term AI
* long-term AI

---
[0]: https://www.lesswrong.com/posts/zB3ukZJqt3pQDw9jz/ai-will-change-the-world-but-won-t-take-it-over-by-playing-3#Technical_Analysis:~:text=Mechanism%201%3A%20Shifting%20horizon%20length%20in%20response%20to%20short%2Dhorizon%20tampering