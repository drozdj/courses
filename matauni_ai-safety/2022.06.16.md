# Stephen M. Omohundro, 2008, “The basic AI drives”, Proceedings of the 2008 conference on Artificial General Intelligence

---

* >They will devote significant analysis to understanding the consequences of modifications before they make them. But once they find an improvement they are confident about, they will work hard to make it happen.
    * consider that it is able to find & evaluate (via exploration) all of the 'modifications' that can allow for its greated success. but not act upon them in training but only in test time.
        * it may not do this to deceive intentially but rather because it hadn't found the most optimal string of actions at the time of *termination* of training.
            * it should also be considered, is it ever approriate to *finish* training? how can we be certain that another phenomenon (like grokking) doesn't happen further in training..?

* >If the runtime environment of the system does not allow it to modify its own machine code, it will be motivated to break the protection mechanisms of that runtime. 
    * but are we not limiting an agent to only a *tuple* of actions/states/rewards..? if not then why?

* >For example, a chess playing robot will not know in advance how much of an improvement it will gain by spending time studying a particular opening move. One way to evaluate an uncertain outcome is to give it a weight equal to its expected utility (the average of the utility of each possible outcome weighted by its probability). The remarkable “expected utility” theorem of microeconomics says that it is always possible for a system to represent its preferences by the expectation of a utility function unless the system has “vulnerabilities” which cause it to lose resources without benefit [1]. 
    * what is being said in the above? it seems very important

--- 
### Key Points
* to stop an agent from self-improvement is a mistake. and that we should encourage this ability, the same way that self-imporvement has been vital to all living species survival.
*  