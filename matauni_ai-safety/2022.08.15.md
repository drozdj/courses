# Nate Soares, 2022, “On how various plans miss the hard bits of the alignment challenge”, LessWrong post.

---
## Owen Cotton-Barratt & Truthful AI 

## Ryan Greenblatt & Eliciting Latent Knowledge
latent knowledge: the 'hidden' information which contributed to a model forming an output but not exposing it. this can be due to it intentially being deceptive (humans may reinforce this behaviour)

## Eric Drexler & AI Services

## Evan Hubinger, in a recent personal conversation

## A fairly straw version of someone with technical intuitions like Richard Ngo’s or Rohin Shah’s

## Vivek Hebbar, summarized (perhaps poorly) from last time we spoke of this in person

## John Wentworth & Natural Abstractions

## Neel Nanda & Theories of Impact for Interpretability
Q: Nate feels very strongly about interpretability work, why is that?

## Stuart Armstrong & Concept Extrapolation

## Andrew Critch & political solutions
