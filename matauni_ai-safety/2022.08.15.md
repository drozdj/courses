# Nate Soares, 2022, “On how various plans miss the hard bits of the alignment challenge”, LessWrong post.

Updated: 31.07.24

Status: Draft

---
## Summary
Nate has a large emphasis on solving the *Hard Problem*, and that you cannot succeed dodging or cheating around it.
In addition, he pairs it with the *sharp left turn* (as this is what he claims to be the 'moment of truth').

---
## Owen Cotton-Barratt & Truthful AI 
* methods for truthful ai
    * language modelling
        * their argument is data (== implicit knowledge) > more truthful
        * choice of prompt ("prompt engineering")
            * it is obvious that depending on the angle that a question is asked to GPT, you will receive a myriad of different answers.
        * finetuning on small datasets thtat reward truthfulness

## Ryan Greenblatt, Paul Christiano & Eliciting Latent Knowledge
* latent knowledge: the 'hidden' information which contributed to a model forming an output but not exposing it. this can be due to it intentially being deceptive (humans may reinforce this behaviour)
* basically, its all fine and dandy to have ELK heads,until there is a *sharp left turn*.

## Eric Drexler & AI Services
Eric suggests to build aweful many narrow-ais that are unable to generalize beyond their domain & are siloed from each other. 
This is actually a idea that i have come across previously.  

* Nate argues that rather than tackling the *Hard Problem*, this seems to avoid it (which is why it is unlikely to work).

## Evan Hubinger, in a recent personal conversation
Evans idea is to find the training processes which are both "capable and aligned" (even through the sharp left turn). then release it to the public, because "it's not like people will be trying to disalign their AIs".
* errors i see in the above
    * who is the decider of which model is "capable and aligned"?
    * wouldn't this promote corruption? (no startups would have the compute to get such a thing working)
    * aren't you creating a behemouth that we can easily loose control over?
    * how do we iterate on such a large model? 

## A fairly straw version of someone with technical intuitions like Richard Ngo’s or Rohin Shah’s
* Rohin Shah
    * > RONIN: "(For the reader: I am not saying "we're screwed if the sharp left turn happens so we should ignore it", I am saying that the sharp left turn is unlikely.)"
    * > ROHIN: "..ex. my approaches include mechanistic interpretability which as you agree could in theory get to that point even if they aren't likely to in practice)."
* Richard Ngo

## Vivek Hebbar, summarized (perhaps poorly) from last time we spoke of this in person
it seems to me like this is similar to Evan Hubinger's view, but it adds atop of it to include knowing the one true objective function. this seems to be a more difficult way than Evans, as because Evan haden't exclusively mentioned it, i conclude that the "capable and aligned" model that he is referring to is not "the one" but many different ways to converge to it.
**Q: battle Vivek and Evans approach.**

## John Wentworth & Natural Abstractions
Johns idea is that the world is madeup of these black-boxes for us humans, when we talk of cars we very rarely know what they comprise of to work. we sort of just, get inside of them, know the basic principles of how to operate them. until what inevitably happens, a flat tire in the middle of the highway, in which case *some* people can solve the problem rather quickly, the rest do not know the 1st step to proceed. 
The point being made is thanks to abstractions, even the most *dull* are able to communicate about, use, operate, such complex systems.

## Neel Nanda & Theories of Impact for Interpretability
Q: Nate feels very strongly about interpretability work, why is that?
    * Nate believes that although interpretability is very useful (the process of understanding how individual neurons in NNs interact to produce an output). However, he also acknoledges that this is not the be-all-end-all to understanding AGI, and that it should work as an *assistant* to "a force-multiplier that awaits some other plan for adressing the hard problems".

## Stuart Armstrong & Concept Extrapolation
> "We would point at all living humans in the world and say "these are humans". Then we would instruct the AI: "please extrapolate the concept of 'human' from this data" <br> -Different perspectives on concept extrapolation (LessWrong) 
* i am unsure that this is a good idea. what if they are curious as to investigating the *black-boxes* that John Wentworth presented earlier but to *know* everything about them?
    * once the external physical representation has been learned, wouldn't they then proceed to tearing humans apart to 'learn' more about their nature to 'Extrapolate Concepts'?

> "The ideal is if the AI implicitly generates "shoplifters", "racial groups", and "clothes style" as separate classifiers. And then enquires, using active learning, as to what its purpose actually is. This allows the AI to classify properly for the purposes that it was designed for - and only those purposes."

* a common theme i am finding while researching Stuart Armstrongs work is that of the agent having ability to *interact* with its own goals, thereby compartmentalizing, prioritizing, and overall  

## Andrew Critch & political solutions
Andrew wants some sort of a global peace going in which all AI Safety community work together.
This seems far-fetching (and *wayy* out of reach) for Nate, but he would love for this to happen.
