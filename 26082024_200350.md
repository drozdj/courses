# [An Extremely Opinionated Annotated List of My Favourite Mechanistic Interpretability Papers v2 by Neel Nanda 7th Jul 2024](https://www.lesswrong.com/posts/NfFST5Mio7BCAQHPA/an-extremely-opinionated-annotated-list-of-my-favourite)

### ?..?
* [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html)
    * [myÂ youtube tutorial](https://www.youtube.com/watch?v=KV5gbOmHbjU&list=PL7m7hLIqA0hpsJYYhlt1WbHHgdfRLM2eY&index=1&pp=gAQBiAQB)

### Superposition
* [Toy models of superposition (Nelson Elhage et al, Anthropic)](https://transformer-circuits.pub/2022/toy_model/index.html)
* [Finding Neurons In A Haystack (Wes Gurnee et al, during my MATS program)](https://arxiv.org/pdf/2305.01610.pdf)
* [Fact Finding](https://www.lesswrong.com/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall)

### Sparse Autoencoders
* [Towards Monosemanticity (Trenton Bricken et al, Anthropic)](https://transformer-circuits.pub/2023/monosemantic-features/index.html)
* [Sparse Feature Circuits (Sam Marks et al, David Bauâ€™s group)](https://arxiv.org/abs/2403.19647)
* [Transcoders Find Interpretable LLM Feature Circuits (Jacob Dunefsky, Philippe Chlenski et al, during my MATS program)](https://arxiv.org/abs/2406.11944)
* [Interpreting Attention Layer Outputs with Sparse Autoencoders (Connor Kissane & Rob Krzyzanowski et al, during my MATS program)](https://arxiv.org/abs/2406.17759)
* [Towards principled evaluations of sparse autoencoders for interpretability and control (Alex Makelov & Georg Lange et al, during my MATS program).](https://arxiv.org/abs/2405.08366)
* [Gated SAEs (Sen Rajamanoharan et al, from my team at DeepMind):](https://arxiv.org/abs/2404.16014)
* [Scaling and evaluating sparse autoencoders (Leo Gao et al, from the OpenAI superalignment team RIP ðŸ˜¢)](https://cdn.openai.com/papers/sparse-autoencoders.pdf)
* [Scaling monosemanticity (Adly Templeton et al, Anthropic)](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)
### Activation Patching
* [How to use and interpret activation patching (Stefan Heimersheim & me)](https://arxiv.org/abs/2404.15255)
* [Causal scrubbing (Redwood)](https://www.lesswrong.com/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing)
* [Attribution Patching (me, work mostly done at Anthropic)](https://www.neelnanda.io/mechanistic-interpretability/attribution-patching)
* [Automated Circuit Discovery (Arthur Conmy et al)](https://arxiv.org/abs/2304.14997)
* [Distributed Alignment Search (Atticus Geiger et al)](https://arxiv.org/abs/2303.02536)
* [An Interpretability Illusion for Subspace Activation Patching (Aleksander Makelov & Georg Lange, during my MATS program)](https://arxiv.org/abs/2311.17030#:~:text=An%20Interpretability%20Illusion%20for%20Subspace%20Activation%20Patching,-Aleksandar%20Makelov%2C%20Georg&text=Mechanistic%20interpretability%20aims%20to%20understand,low%2Ddimensional%20subspaces%20of%20activations.)

### Narrow Circuits
* [Indirect Object Identification (Kevin Wang et al, Redwood)](https://arxiv.org/abs/2211.00593)
* [A Greater-Than Circuit (Michael Hanna et al, Redwood)](https://arxiv.org/abs/2305.00586)
* [Does Circuit Analysis Interpretability Scale? (Tom Lieberum et al, DeepMind)](https://arxiv.org/abs/2307.09458)

### Extra
* [The induction heads paper (Catherine Olsson et al, Anthropic)](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)
* [Progress Measures for Grokking via Mechanistic Interpretability (Neel Nanda et al)](https://arxiv.org/abs/2301.05217)
* [Logit Lens (nostalgebraist)](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens)
* [Activation Addition (Alex Turner et al, done with his MATS scholars before joining Google DeepMind)](https://arxiv.org/abs/2308.10248)
* [Inference-Time Interventions (Kenneth Li et al, Harvard)](https://arxiv.org/abs/2306.03341)
* [Representation Engineering (Andy Zou et al, a CAIS project)](https://arxiv.org/abs/2310.01405)
* [Refusal is Mediated by A Single Direction (Andy Arditi et al, during my MATS program)](https://arxiv.org/abs/2406.11717)
* [The Hydra Effect (Tom McGrath et al, Google DeepMind)](https://arxiv.org/abs/2307.15771)
* [Explorations of Self-Repair in Language Models (Cody Rushing et al, during my MATS program)](https://arxiv.org/abs/2402.15390v1)
* [Copy Suppression (Callum McDougall, Arthur Conmy & Cody Rushing et al, during my MATS program)](https://arxiv.org/abs/2310.04625)
* [Linear Representations of Sentiment (Curt Tigges & Oskar Hollinsworth et al, during my MATS program)](https://arxiv.org/pdf/2310.15154.pdf)
* [Softmax Linear Units (Nelson Elhage et al, Anthropic)](https://transformer-circuits.pub/2022/solu/index.html)
* [Language models can explain neurons in language models (Steven Bills et al, OpenAI)](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html)
* [An Interpretability Illusion for BERT (Tolga Bolukbasi et al, Google)](https://arxiv.org/pdf/2104.07143.pdf)
* [Multimodal Neurons in Artificial Neural Networks (Gabriel Goh et al, OpenAI)](https://distill.pub/2021/multimodal-neurons/)