# 2022 Alignment Fundamentals
https://docs.google.com/document/d/1mTm_sT2YQx3mRXQD6J2xD2QJG1c3kHyvX8kQc_IQ0ns/edit#heading=h.dlm795ug69gc


## Week 0 (optional): Introduction to machine learning 
1. A short introduction to machine learning (Ngo, 2021) (20 mins) 
2. But what is a neural network? (3Blue1Brown, 2017a) (20 mins) 
3. Gradient descent, how neural networks learn (3Blue1Brown, 2017b) (20 mins) 
4. Creating a Space Game with OpenAI Codex (OpenAI, 2021) (10 mins) 
5. Introduction to reinforcement learning (von Hasselt, 2021) (from 2:00 to 1:02:10, ending at the beginning of the section titled Inside the Agent: Models) (60 mins) 

## Week 1: Artificial general intelligence 
1. Four background claims (Soares, 2015) (15 mins) (note that claims #3 and #4 are covered in more detail in the following two weeks) 
2. AGI safety from first principles (Ngo, 2020) (only sections 1 and 2) (20 mins) 
3. The Bitter Lesson (Sutton, 2019) (5 mins) 
4. Future ML systems will be qualitatively different (Steinhardt, 2022) (5 mins) 
5. Forecasting transformative AI: the “biological anchors” method in a nutshell (Karnofsky, 2021b) (30 mins) 

## Week 2: Goals and misalignment 
1. Specification gaming: the flip side of AI ingenuity (Krakovna et al., 2020) (15 mins) 
2. Goal misgeneralization in deep reinforcement learning (Langosco et al., 2022) (ending after section 3.3) (25 mins) 
    1. Those with less background in reinforcement learning can skip the parts of section 2.1 focused on formal definitions. 
3. Superintelligence, Chapter 7: The superintelligent will (Bostrom, 2014) (25 mins) 
4. The alignment problem from a deep learning perspective (Ngo, 2022) (only sections 2, 3 and 4) (25 mins) 

## Week 3: Threat models and types of solutions 
1. Intelligence explosion: evidence and import (Muehlhauser and Salamon, 2012) (only pages 10-15) (15 mins) 
2. What failure looks like (Christiano, 2019) (20 mins) 
3. ML systems will have weird failure modes (Steinhardt, 2022) (15 mins) 
4. AGI safety from first principles (Ngo, 2020) (only section 5: Control) (15 mins) 
5. AI alignment landscape (Christiano, 2020) (35 mins) 

## Week 4: Learning from humans 
1. Imitation learning lecture: part 1 (Levine, 2021a) (20 mins) 
2. Read all four of the following blog posts, plus the full paper for whichever you found most interesting (if you’re undecided, default to the critiques paper): 
    1. Deep RL from human preferences: blog post (Christiano et al., 2017) (10 mins) 
    2. Aligning language models to follow instructions: blog post (Ouyang et al,, 2022) (10 mins) 
    3. AI-written critiques help humans notice flaws: blog post (Saunders et al., 2022) (10 mins) 
    4. Red-teaming language models with language models (Perez et al., 2022) (10 mins) 
3. The easy goal inference problem is still hard (Christiano, 2015) (10 mins) 

## Week 5: Decomposing tasks for better supervision  

1. Summarizing books with human feedback: blog post (Wu et al., 2021) (5 mins) 
2. Factored cognition (Ought, 2019) (introduction and scalability section) (20 mins) 
3. Supervising strong learners by amplifying weak experts (Christiano et al., 2018) (35 mins) 
4. AI safety via debate (Irving et al., 2018) (ending after section 3) (35 mins) 
    1. Those without a background in complexity theory can skip section 2.2. 
5. Debate update: obfuscated arguments problem (Barnes and Christiano, 2020) (excluding appendix) (15 mins) 

## Week 6: Interpretability 
1. Feature visualization (Olah et al, 2017) (20 mins) 
2. Zoom In: an introduction to circuits (Olah et al., 2020) (35 mins) 
3. Mechanistic interpretability, variables, and the importance of interpretable bases (Olah, 2022) (10 mins) 
4. Locating and Editing Factual Associations in GPT: blog post (Meng et al., 2022) (10 mins) 
5. Acquisition of chess knowledge in AlphaZero (McGrath et al., 2021) (only up to the end of section 2.1) (20 mins) 

## Week 7: Agent foundations, AI governance, and careers in alignment 
1. Embedded agents, part 1 (Demski and Garrabrant, 2018) (15 mins) 
2. Read one of the following three blog posts, which give brief descriptions of work on agent foundations. 
    1. Logical induction: blog post (Garrabrant et al., 2016) (10 mins) 
    2. Finite factored sets: talk transcript (Garrbarant, 2021) (only sections 2m: the Pearlian paradigm and 2t: we can do better) (10 mins) 
3. Progress on causal influence diagrams: blog post (Everitt et al., 2021) (15 mins) 
3. AI Governance: Opportunity and Theory of Impact (Dafoe, 2020) (25 mins) 
4. Cooperation, conflict and transformative AI: sections 1 & 2 (Clifton, 2019) (25 mins) 
5. Careers in alignment (Ngo, 2022) (30 mins)

## Week 8 (four weeks later): Projects 
1. Technical upskilling - e.g. training neural networks, or reimplementing papers. 
2. Distilling understanding - e.g. summarizing existing work, or doing literature reviews. 
3. Novel exploration - e.g. black-box investigation of language models. 
