# Vaswani et al. - 2023 - Attention Is All You Need

--- 

--- --- ---
### TAKE0

Q: why is this paper important & why should i spend my time reading it?  
* Prior to the release, CNN & RNN ‘s were used to train tasks such as *machine translation, query-answer chat bots.  

The limitation was that they relied on LSTM and ___ which was *slow* and had to process input *sequencially (which often times was jumbled).  
The introduction of the Transformer allowed for input to be processed  

--- --- ---
### TAKE1

how were CNN & RNN 's able to do these *machine translation* tasks? i've only seen CNN's used to 'fade' across a image to take many pixels in that frame and perform some compute to it (ex. downscaling).
* foo

however, for RNN's what are they actually used for? 
* foo

they loop onto themselves but what do they then do with that information?
* foo

how is it that *transformers* do not process these tokens sequentially?
* foo

what is a token?
* foo


[lets take a look what happens to RNN's when they are actually tasked with *machine translation*.](https://medium.com/@zaiinn440/from-seq2seq-to-attention-revolutionizing-sequence-modeling-67282ba82e83)
* "2 RNNs fails drastically with increasing sentence length."
    * we are not off to a good start...
        * "It is not able to capture all the relevant information in big sequences." 
            * foo

the transformer was introduced to tackle CNN and RNN 's [struggle with dealing with long inputs](https://www.linkedin.com/pulse/attention-all-you-need-ryan-s-/). 
come to find out, transformers have also fell victim to this [due to complexity](https://papers.nips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Reviews.html)


The Scaled Dot-Product
Multi-head Attention
Positional Encoding
Transformer Architecture
Residual connections & Layer normalization
Feed-Forward Network
The encoder
The decoder

* [ ] https://www.youtube.com/watch?v=dichIcUZfOw
* [ ] https://medium.com/@zaiinn440/attention-is-all-you-need-the-core-idea-of-the-transformer-bbfa9a749937
* [ ] https://jalammar.github.io/illustrated-transformer/
* [x] https://drlee.io/an-intuitive-explanation-of-attention-is-all-you-need-the-paper-that-revolutionized-ai-and-39aac5827411
    * great resource
* [x] https://luv-bansal.medium.com/transformer-attention-is-all-you-need-easily-explained-with-illustrations-d38fdb06d7db
* [x] https://www.linkedin.com/pulse/attention-all-you-need-ryan-s-/
* [ ] https://research.google/blog/transformer-a-novel-neural-network-architecture-for-language-understanding/
* [ ] https://medium.com/data-science-in-your-pocket/attention-is-all-you-need-understanding-with-example-c8d074c37767
* [ ] https://towardsdatascience.com/transformer-attention-is-all-you-need-1e455701fdd9
* [ ] https://medium.com/@adityathiruvengadam/transformer-architecture-attention-is-all-you-need-aeccd9f50d09

too-advanced for now (but great resource)
* [ ] https://www.youtube.com/watch?v=iDulhoQ2pro
--- --- ---
### Other

“Recurrent neural networks (RNN), long short-term memory networks(LSTM) and gated RNNs are the popularly approaches used for Sequence Modelling tasks such as machine translation and language modeling. However, RNN/CNN handle sequences word-by-word in a sequential fashion. This sequentiality is an obstacle toward parallelization of the process. Moreover, when such sequences are too long, the model is prone to forgetting the content of distant positions in sequence or mix it with following positions’ content.“  
[https://hub.packtpub.com/paper-in-two-minutes-attention-is-all-you-need/](https://hub.packtpub.com/paper-in-two-minutes-attention-is-all-you-need/)

--- --- ---
What is being suggested it that


**Reviews**

“Traditional neural network models for NLP tasks, such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs), rely on fixed-length representations of the input sequence.“ [https://iq.opengenus.org/attention-is-all-you-need-summary/](https://iq.opengenus.org/attention-is-all-you-need-summary/)

“attention-based models are superior to traditional models for NLP tasks because they allow the model to selectively attend to different parts of the input sequence“

“The model consists of an encoder and a decoder, both of which are composed of multiple layers of self-attention and feedforward neural networks“

**unfamiliar terms  
encoder.  
decoder.**