# Acquisition of chess knowledge in AlphaZero (McGrath et al., 2021)

--- 


<!-- todo: -->
* [ ] mindmap encode
    * concept-based interpretability
        * network probing (on pre-trained models)
    * 

> If the representations of strong neural networks bear no resemblance to human concepts, our ability to understand faithful explanations of their decisions will be restricted, ultimately limiting what we can achieve with neural network interpretability. 

>Notable examples include single neurons in a classifier corresponding to mountain tops or snow [1]
<!-- todo: -->

> (1) build an inherently interpretable model 
* is this done before/during/after training?
    * we likely build the model around it being interpretable. 
        * how can we go about doing this? 
            * <!-- todo: -->
* approach
    * probe for human concepts
        * organizations in chess such as queens gambit, london gambit, etc
    * behaviour changes
        * during training, before entirely converging to the most optimal, the bot has chains of thought that mostly 
        build early and strenghthed at the end of training. 
            * why is this?
                * <!-- todo: -->
    * investigate layers & activations directly
        * this goes into the nitty gritty diving beyond human expalinable concepts  
