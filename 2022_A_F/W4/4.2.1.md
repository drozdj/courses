# Deep RL from human preferences: blog post (Christiano et al., 2017)

---
* what happens when the *terminal goal* is large in size?
* what if the human evaluator is wank?
* for every state, how many 'actions' are presented for us to pick the *most* optimal? 
* what if down the line, the evaluator thinks to themselves, "there was a much better action i 
could have taken '...' moves ago?"
* whats the paper about?
    * trained agent to do a backflip
        * human expert shown many variations of the next action(s?), and chooses the most appropriateleading to the goal.
        * surely many actions are *merged* together, as in an environment like this there must be infite number of states for the agent to transition to...?
            * >between pairs of trajectory segments.
            * > On most of the games real human feedback performs similar to or slightly worse than synthetic feedback with the same number of labels, and often comparably to synthetic feedback that has 40% fewer labels. This may be due to human error in labeling, inconsistency between different contractors labeling the same run, or the uneven rate of labeling by contractors, which can cause labels to be overly concentrated in narrow parts of state space. The latter problems could potentially be addressed by future improvements to the pipeline for outsourcing labels.
            * >In general we discovered that asking humans to compare longer clips was significantly more helpful per clip, and significantly less helpful per frame.     
                <br>
            * what happens when you have a 'dumb' human expert?
                * > Our algorithm’s performance is only as good as the human evaluator’s intuition about what behaviors look correct, so if the human doesn’t have a good grasp of the task they may not offer as much helpful feedback
                    * thats not very helpful, what is done to address it?
                    <br>
            * on a larger scale, aren't human's as experts not **significantly** more costly than machines?
                * <!-- todo: answer above -->
                <br>
            * what if the human expert themselves aren't able to complete the very task that the robot should learn?
                * <!-- todo: answer above-->


---
### Key Points
