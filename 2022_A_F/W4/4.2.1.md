# Deep RL from human preferences: blog post (Christiano et al., 2017)

---

* what happens when the *terminal goal* is large in size?

* what if the human evaluator is wank?

* for every state, how many 'actions' are presented for us to pick the *most* optimal? 

* what if down the line, the evaluator thinks to themselves, "there was a much better action i 
could have taken '...' moves ago?"

* whats the paper about?
    * trained agent to do a backflip
        * human expert shown many variations of the next action(s?), and chooses the most appropriate
        leading to the goal.
            * surely many actions are *merged* together, as in an environment like this there must be
            infite number of states for the agent to transition to...?
                * <!-- todo: answer above -->
            * what happens when you have a 'dumb' human expert?
                * <!-- todo: answer above --> 
            * on a larger scale, aren't human's as experts not **significantly** more costly than machines?
                * <!-- todo: answer above -->
            * what if the human expert themselves isn't able to complete the very task that the robot
            should learn?
                * <!-- todo: answer above-->


---
### Key Points
