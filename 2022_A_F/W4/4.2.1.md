# Deep RL from human preferences: blog post (Christiano et al., 2017)

---

* what happens when the *terminal goal* is large in size?

* what if the human evaluator is wank?

* for every state, how many 'actions' are presented for us to pick the *most* optimal? 

* what if down the line, the evaluator thinks to themselves, "there was a much better action i 
could have taken '...' moves ago?"

---
### Key Points
