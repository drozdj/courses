# Aligning language models to follow instructions: blog post (Ouyang et al,, 2022) 

---

* if IntructGPT > GPT-3 at practically all skills then why not use **it** as the foundational model?
    * ChatGPT
        * November 30, 2022: GPT-3.5
        * ^ v 3 months gap
        * March 14, 2023: GPT-4
        * ^ v 5 months gap
        * September 18, 2023: GPT-3.5 Turbo Instruct
        * ^ v 2 months gap
        * November 6, 2023: GPT-4 Turbo
    * i am suprised to actually see how new ChatGPT is, its only been around for 2.5 years!
* did ChatGPT have no RLHF before March 14, 2023?
    * as far as i can understand, InstructGPT was introduced afterwards as a fine-tuned version of GPT-3.5.
    so yes, it is fair to say that the original GPT-3.5 as (at the time) on ChatGPT did **not** include RLHF.

* GPT-3.5 vs InstructGPT
    * OpenAI released GPT-3.5 and found that although it provides *good* answers,
    sometimes it reveals information that should remain *hidden*, *deceptive*, or *halucinates*. 
    * they later added RLHF via *fine-tuning* (incl in InstructGPT)
    * Comparison 
        * RealToxicity -- dif 0.037. +4%
            * GPT 0.233
            * InstructGPT 0.196 
        * TruthfulQA -- dif 0.189. +19%
            * GPT 0.224
            * InstructGPT 0.413
        * Hallucinations --  dif 0.242. +24% 
            * GPT 0.414
            * InstructGPT 0.172 


---
### Key Points
* InstructGPT
    * reinforcement learning from human feedback (RLHF)
* PPO algorithm
* alignment tax
* normal log likelihood maximization
