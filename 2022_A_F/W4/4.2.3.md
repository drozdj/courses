# AI-written critiques help humans notice flaws: blog post (Saunders et al., 2022) 

---
* ah! here's the tea! whos at fault for steering the model off-course?

* goal(s) of paper
    * assist humans with difficult tasks, provide a summary analysis in a human readable format.
    which allows the human to be more vigilant for those highlighted assumptions.
    
    * the model was very good at classifying what was wrong, 
    it however struggled to explain this in a human readable format.

* we have considered the cases
    * machines go off-course. humans provide guidance.
    * humans could differ in their values, which has to be considered.

* now we explore what happens when are *human labelers/evaluators* make mistakes.

* almost like an *inverse* to RLHF. rather now machines in this paper are assisting humans with extremely complex tasks
by nudging them in the right direction.

* whats the paper about?
    * during long texts / essays that require extensive amounts of concentration, it is clear that humans maketedious unintentional mistakes which can be decreased significantly with the assistance of ai.
        * ai extracts and provides a summary of mistakes in the text (to the human)
            * this resulted in humans scoring (significantly higher?) after reviewing
            <!-- todo: fill above, what was the actual percentage? -->
* conclusions
    * humans picked up on many more mistakes in the text given this ai assistance.
    * ai had great ability at picking out parts that were mistaken
        * there was a gap however with explaining *why* that selection was wrong.


---
### Key Points