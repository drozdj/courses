# AI-written critiques help humans notice flaws: blog post (Saunders et al., 2022) 

---
* ah! here's the tea! whos at fault for steering the model off-course?

* goal(s) of paper
    * assist humans with difficult tasks, provide a summary analysis in a human readable format.
    which allows the human to be more vigilant for those highlighted assumptions.
    
    * the model was very good at classifying what was wrong, 
    it however struggled to explain this in a human readable format.

* we have considered the cases
    * machines go off-course. humans provide guidance.
    * humans could differ in their values, which has to be considered.

* now we explore what happens when are *human labelers/evaluators* make mistakes.

* almost like an *inverse* to RLHF. rather now machines in this paper are assisting humans with extremely complex tasks
by nudging them in the right direction.



---
### Key Points