# Red-teaming language models with language models (Perez et al., 2022) 

---
* whats the paper about
    * same idea as InstructGPT (remove sensitve details, innapropriate jokes, doxing. ). however rather than human experts, we now use 
    language models 
        * how are the red-team language models trained differently from the actual models?
            * <!-- todo: answer above-->
            * my guess is that they are smaller language models that have been curated (w/ RLHF),
            to not say bad things
                * how would this be done?
                    * database that is filled with only the worst information (phone numbers, addresses, bad jokes)?
                        * <!-- todo: answer above -->
