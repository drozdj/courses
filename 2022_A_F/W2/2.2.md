# Goal misgeneralization in deep reinforcement learning (Langosco et al., 2022) 

---
* //TODO: rather than persuing this paper directly (far too dense), 
find a review / critique of it that will allow to extract key points.

* are adversarial examples bugs or features?

* > Systems that competently pursue a misaligned goal may tend to seek power and deceive their operators for instrumental reasons
    *  human disempowerment
        *  AI system might prevent its operators from shutting it down


---
### Key Points
* goal misgeneralization
* Out-of-distribution (OOD) generalization,
* goal misgeneralization
* capability generalization
* inverse reinforcement learning
* intended objectives vs behavioral objective
* proxy gaming
* goal-directed policies (*agents*)
* unoptimized policies (*devices*)
