# Goal misgeneralization in deep reinforcement learning (Langosco et al., 2022) 

---

* are adversarial examples bugs or features?

* > Systems that competently pursue a misaligned goal may tend to seek power and deceive their operators for instrumental reasons
    *  human disempowerment
        *  AI system might prevent its operators from shutting it down


---
### Key Points
* goal misgeneralization
* Out-of-distribution (OOD) generalization,
* goal misgeneralization
* capability generalization
* inverse reinforcement learning
* intended objectives vs behavioral objective
* proxy gaming
* goal-directed policies (*agents*)
* unoptimized policies (*devices*)
