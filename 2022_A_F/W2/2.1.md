# Specification gaming: the flip side of AI ingenuity (Krakovna et al., 2020) 

---
*  whats this paper about? 
    * **agents may learn to achieve the tasks we set for them but not in the intended way.**
        * several atari games have been shown that when trained with RL. agents tend to find *exploits*
        to receive rewards without actually ever completing the goal we intended
            * this may occur if we set the agent to persue the wrong metric.
                * "maximize score"
                    * agent sinks all of the enemies ships to *proxy game*
                    * agent picks up score boost, then offs itself so that it can respawn and retreive it in a loop
                    * robot arm hides ball behind in its hands and video recognition classifies the task as completed successfully

* > "Any task specification has a physical manifestation: a reward function stored on a computer, or preferences stored in the head of a human. An agent deployed in the real world can potentially manipulate these representations of the objective, creating a reward tampering problem. "
    * its discussed how a agent has the potential to alter *reward functions* once it is deployed in the real world via *tampering problem*.
    when in a sandbox, where everything is stored on a computer and we have control of it, everything runs smoothly.
        * we have to however consider that these models will be run independently as a robot, for which we cannot then make changes. and now *it* is able to have control
        over itself.

--- 
### Key Points
* Specification gaming
* task specification

---
### Further Readings
* [The list of specification gaming behaviours](https://deepmind.google/discover/blog/specification-gaming-the-flip-side-of-ai-ingenuity/)

